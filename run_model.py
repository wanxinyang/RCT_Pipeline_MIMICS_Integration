"""
================================================================================
MIMICS Model Execution Script 
================================================================================
This script runs the MIMICS model with parameter sets from
a CSV file generated by create_parameters.py.

Main workflow:
1. Load CSV file with MIMICS input parameters
2. For each parameter set (in parallel):
   - Create isolated temporary directory
   - Configure MIMICS input files with parameters
   - Execute MIMICS model
   - Parse backscatter output results
3. Combine all results into single output CSV
4. Optionally preserve individual tree results

Authors: Russell Thomas and Wanxin Yang
Last update date: 2025-11-05
================================================================================
"""

# ============================================================================
# IMPORTS
# ============================================================================
import pandas as pd
import subprocess
import re
import shutil
import argparse
import time
import logging
import signal
import sys
import os
import tempfile
from pathlib import Path
from multiprocessing import Pool, cpu_count
from tqdm import tqdm
from datetime import datetime

# ============================================================================
# SIGNAL HANDLING
# ============================================================================
# Global flag for graceful shutdown
shutdown_requested = False

def signal_handler(signum, frame):
    """Handle interrupt signals gracefully"""
    global shutdown_requested
    print("\n\nShutdown requested. Finishing current runs and cleaning up...")
    shutdown_requested = True

# ============================================================================
# MIMICS PARAMETER MAPPING
# ============================================================================
# Parameter locations in MIMICS input files
# Format: 'parameter_name': ('input_file', template_line_number, value_line_number)
PARAMETERS = {
    # Sensor parameters
    'frequency': ('sensor.input', 13, 14),                    # Radar frequency (GHz)
    'angle': ('sensor.input', 20, 21),                        # Look angle (degrees)

    # Ground surface parameters
    'soil_moisture': ('ground.input', 22, 23),               # Volumetric soil moisture (fraction 0-1)
    'rms_height': ('ground.input', 29, 30),                  # RMS surface roughness height (cm)
    'correlation_length': ('ground.input', 36, 37),          # Surface correlation length (cm)
    'percent_sand': ('ground.input', 45, 46),                # Soil sand percentage (%)
    'percent_clay': ('ground.input', 50, 51),                # Soil clay percentage (%)
    
    # Trunk and canopy parameters
    'trunk_moisture': ('trunk_and_gross_canopy.input', 14, 15),      # Trunk gravimetric moisture (fraction 0-1)
    'trunk_dry_density': ('trunk_and_gross_canopy.input', 21, 22),   # Trunk dry wood density (g/cm³)
    'canopy_density': ('trunk_and_gross_canopy.input', 28, 29),      # Tree density (trees/m²)
    'crown_thickness': ('trunk_and_gross_canopy.input', 35, 36),     # Crown thickness (m)
    'trunk_diameter': ('trunk_and_gross_canopy.input', 42, 43),      # Trunk diameter (cm)
    'trunk_length': ('trunk_and_gross_canopy.input', 49, 50),        # Trunk length (m)

    # Primary branch parameters (1st order)
    'branch_1_moisture': ('branch_primary.input', 14, 15),           # Primary branch moisture (fraction 0-1)
    'branch_1_dry_density': ('branch_primary.input', 21, 22),        # Primary branch dry density (g/cm³)
    'branch_1_length': ('branch_primary.input', 28, 29),             # Primary branch length (m)
    'branch_1_diameter': ('branch_primary.input', 35, 36),           # Primary branch diameter (cm)
    'branch_1_density': ('branch_primary.input', 42, 43),            # Primary branch density (branches/m³)
    'branch_1_pdf': ('branch_primary.input', 53, 54),                # Primary branch orientation PDF type (integer)

    # Secondary branch parameters (2nd order)
    'branch_2_moisture': ('branch_secondary.input', 14, 15),         # Secondary branch moisture (fraction 0-1)
    'branch_2_dry_density': ('branch_secondary.input', 21, 22),      # Secondary branch dry density (g/cm³)       
    'branch_2_length': ('branch_secondary.input', 28, 29),           # Secondary branch length (m)
    'branch_2_diameter': ('branch_secondary.input', 35, 36),         # Secondary branch diameter (cm)
    'branch_2_density': ('branch_secondary.input', 42, 43),          # Secondary branch density (branches/m³)
    'branch_2_pdf': ('branch_secondary.input', 53, 54),              # Secondary branch orientation PDF type (integer)

    # Tertiary branch parameters (3rd order)
    'branch_3_moisture': ('branch_3rd.input', 14, 15),               # Tertiary branch moisture (fraction 0-1)
    'branch_3_dry_density': ('branch_3rd.input', 21, 22),            # Tertiary branch dry density (g/cm³)       
    'branch_3_length': ('branch_3rd.input', 28, 29),                 # Tertiary branch length (m)
    'branch_3_diameter': ('branch_3rd.input', 35, 36),               # Tertiary branch diameter (cm)
    'branch_3_density': ('branch_3rd.input', 42, 43),                # Tertiary branch density (branches/m³)
    'branch_3_pdf': ('branch_3rd.input', 53, 54),                    # Tertiary branch orientation PDF type (integer)

    # Quaternary branch parameters (4th order)
    'branch_4_moisture': ('branch_4th.input', 14, 15),               # Quaternary branch moisture (fraction 0-1)
    'branch_4_dry_density': ('branch_4th.input', 21, 22),            # Quaternary branch dry density (g/cm³)       
    'branch_4_length': ('branch_4th.input', 28, 29),                 # Quaternary branch length (m)
    'branch_4_diameter': ('branch_4th.input', 35, 36),               # Quaternary branch diameter (cm)
    'branch_4_density': ('branch_4th.input', 42, 43),                # Quaternary branch density (branches/m³)
    'branch_4_pdf': ('branch_4th.input', 53, 54),                    # Quaternary branch orientation PDF type (integer)

    # Quinary branch parameters (5th order)
    'branch_5_moisture': ('branch_5th.input', 14, 15),               # Quinary branch moisture (fraction 0-1)
    'branch_5_dry_density': ('branch_5th.input', 21, 22),            # Quinary branch dry density (g/cm³)       
    'branch_5_length': ('branch_5th.input', 28, 29),                 # Quinary branch length (m)
    'branch_5_diameter': ('branch_5th.input', 35, 36),               # Quinary branch diameter (cm)
    'branch_5_density': ('branch_5th.input', 42, 43),                # Quinary branch density (branches/m³)
    'branch_5_pdf': ('branch_5th.input', 53, 54),                    # Quinary branch orientation PDF type (integer)
    
    # Leaf parameters
    'leaf_density': ('leaf.input', 41, 42),                          # Leaf density (leaves/m³)
}


# ============================================================================
# MIMICS CONFIGURATION FUNCTIONS
# ============================================================================

def configure_mimics_branches(config_file_path, row_dict):
    """
    Configure which branch orders are enabled in MIMICS configuration.
    
    Args:
        config_file_path: Path to MIMICS configuration file
        row_dict: Dictionary with parameter values
    """
    if not config_file_path.exists():
        return
    
    with open(config_file_path, 'r') as f:
        lines = f.readlines()
    
    # Determine which branches have non-zero density
    branch_enabled = {}
    for i in range(1, 6):  # branches 1-5
        density_key = f'branch_{i}_density'
        branch_enabled[i] = (density_key in row_dict and 
                           pd.notna(row_dict[density_key]) and 
                           row_dict[density_key] > 0)
    
    # Update configuration file to enable/disable branches
    # This is a simplified version - actual implementation depends on MIMICS config format
    modified = False
    for i, line in enumerate(lines):
        if 'Primary Branches' in line and 'T' in line:
            if branch_enabled.get(1, False):
                lines[i] = line.replace('F', 'T') if 'F' in line else line
            else:
                lines[i] = line.replace('T', 'F') if 'T' in line else line
            modified = True
        elif 'Secondary Branches' in line and ('T' in line or 'F' in line):
            if branch_enabled.get(2, False):
                lines[i] = line.replace('F', 'T') if 'F' in line else line
            else:
                lines[i] = line.replace('T', 'F') if 'T' in line else line
            modified = True
        # Add similar logic for 3rd, 4th, 5th branches if needed
    
    if modified:
        with open(config_file_path, 'w') as f:
            f.writelines(lines)

# ============================================================================
# PARAMETER FORMATTING AND SETTING
# ============================================================================

def get_format(filename, template_line_num, value_line_num, target_data_dir):
    """
    Read MIMICS input files to understand exact formatting requirements.
    
    Creates format strings for parameter replacement by analyzing template
    patterns in MIMICS input files.
    
    Args:
        filename: MIMICS input file name
        template_line_num: Line number containing format template (1-indexed)
        value_line_num: Line number containing value to replace (1-indexed)
        target_data_dir: Path to MIMICS data directory
        
    Returns:
        Format string with {value:spec} placeholder, or None if invalid
    """

    # create path to mimics input file
    filepath = target_data_dir / filename
    # opens file and read all lines into a list
    with open(filepath, 'r') as f:
        lines = f.readlines()

    # get the specific lines
    template_line = lines[template_line_num - 1]
    value_line = lines[value_line_num - 1]
    
    # Handle PDF parameters (integer with II pattern)
    if 'II' in template_line:
        ii_match = re.search(r'II+', template_line)
        if ii_match:
            start_pos = ii_match.start()
            end_pos = ii_match.end()
            field_width = len(ii_match.group(0))
            return value_line[:start_pos] + f"{{value:{field_width}d}}" + value_line[end_pos:]
    
    # Handle float parameters (XX.xxx pattern)
    # Finds all float patterns like 'XX.xxx', 'XXX.xx', etc.
    template_pattern = r'X+\.\w+'
    template_matches = list(re.finditer(template_pattern, template_line))
    
    # If no patterns found, return None (invalid template)
    if not template_matches:
        return None
    
    # Start with the original value line and track replacements
    result_line = value_line
    replacements = 0
    
    # Process patterns (reverse order to avoid shifting issue), skip increment setting if 3 patterns exist 
    for i, match in enumerate(reversed(template_matches)):
        if len(template_matches) == 3 and i == 0:
            continue
        if replacements < 2:
            start_pos = match.start()
            end_pos = match.end()
            template_text = match.group(0)
            
            # Get decimal places from template ('XX.X' = 1 decimal place, 'XXX.xxx' = 3 decimal places)
            if '.' in template_text:
                decimal_places = len(template_text.split('.')[1])
            else:
                decimal_places = 0
            
            # Create format specification ('XX.X' (4 chars, 1 decimal) → "{value:4.1f}")
            field_width = len(template_text)
            format_spec = f"{field_width}.{decimal_places}f"
            
            # replace the pattern with format placeholder and increment counter (e.g.: "frequency  0.5 GHz" becomes "frequency {value:4.1f} GHz"            
            result_line = result_line[:start_pos] + f"{{value:{format_spec}}}" + result_line[end_pos:]
            replacements += 1
    
    return result_line

def set_parameters(params, target_data_dir):
    """
    Set parameters in MIMICS input files within the specified data directory.
    
    Args:
        params: Dictionary of parameter_name -> value
        target_data_dir: Path to MIMICS data directory
    """
    # Create empty dictionary to organize which files need changes
    # Structure will be: {filename: [list of modifications]}
    files_to_modify = {}
    
    # Loops through each parameter in the input dictionary 
    for param_name, value in params.items():
        # check if parameter is one we know how to handle (defined in PARAMETERS dictionary)
        if param_name in PARAMETERS:
            # if good, gets file and line numbers for this parameter
            filename, template_line, value_line = PARAMETERS[param_name]
            # If this is the first parameter for this file, create an empty list for it
            if filename not in files_to_modify:
                files_to_modify[filename] = []
            
            # Gets the exact formatting requirements for this parameter. Returns something like "frequency {value:4.1f} GHz"
            format_string = get_format(filename, template_line, value_line, target_data_dir)
            # If formatting worked, add this modification to the file's list (Stores: (line_number, format_string, actual_value))
            if format_string:
                files_to_modify[filename].append((value_line, format_string, value))
    
    # Second Loop - Apply Changes:
    # Process each MIMICS file that needs changes
    for filename, modifications in files_to_modify.items():
        # Read all lines from the current file into memory
        filepath = target_data_dir / filename
        with open(filepath, 'r') as f:
            lines = f.readlines()
        # Apply each modification for this file
        for value_line, format_string, value in modifications:
            # If this is an integer parameter (PDF), convert to integer
            if 'd}' in format_string:
                value = int(value)
            # Replace the specific line with the formatted value (value_line - 1 converts line number to array index)
            lines[value_line - 1] = format_string.format(value=value)
        
        with open(filepath, 'w') as f:
            f.writelines(lines)

# ============================================================================
# MIMICS MODEL EXECUTION
# ============================================================================

def run_model(src_dir, config_dir, results_dir):
    """
    Execute the MIMICS model with specified directories.
    
    Args:
        src_dir: Path to MIMICS source code directory
        config_dir: Path to configured input parameters directory
        results_dir: Path to save output results
        
    Returns:
        True if execution successful, False otherwise
    """
    # MIMICS uses hardcoded relative paths: ../data/ and ../results/
    # Use the same approach as the original script: create a complete temporary structure
    
    with tempfile.TemporaryDirectory() as temp_work_dir:
        temp_work_path = Path(temp_work_dir)
        
        # Create the same structure as the original script:
        # temp_work_dir/
        #   code/           <- Complete copy of source directory
        #   data/           <- Points to our config_dir (copy or symlink)
        #   results/        <- Points to our results_dir (copy or symlink)
        
        temp_code_dir = temp_work_path / "code" 
        temp_data_dir = temp_work_path / "data"
        temp_results_dir = temp_work_path / "results"
        
        # Copy the entire code directory (like the original)
        shutil.copytree(src_dir, temp_code_dir)
        
        # For data: symlink to our custom config directory
        temp_data_dir.symlink_to(config_dir.resolve())
        
        # For results: symlink to our custom results directory  
        temp_results_dir.symlink_to(results_dir.resolve())
        
        # Debug: Log the directories being used
        logging.debug(f"MIMICS execution structure:")
        logging.debug(f"  temp_work_dir: {temp_work_path}")
        logging.debug(f"  code: {temp_code_dir} (copied from {src_dir})")
        logging.debug(f"  data -> {config_dir}")
        logging.debug(f"  results -> {results_dir}")
        
        # Run MIMICS from the code directory (same as original)
        process = subprocess.run(
            ['bash', '-c', f'cd {temp_code_dir} && echo "go" | ./mimics1.5'],
            capture_output=True, 
            text=True,
            timeout=300  # 5 minute timeout
        )
        
        # Always log process output for debugging
        if process.stdout:
            logging.debug(f"MIMICS STDOUT: {process.stdout}")
        if process.stderr:
            logging.debug(f"MIMICS STDERR: {process.stderr}")
        
        if process.returncode != 0:
            logging.error(f"MIMICS execution failed with return code {process.returncode}")
            return False
        
        # Check if any output files were created in the expected location
        expected_files = ['forest_sigma_like.out', 'forest_sigma_cross.out']
        found_files = []
        for filename in expected_files:
            if (results_dir / filename).exists():
                found_files.append(filename)
        
        logging.debug(f"MIMICS output files found in {results_dir}: {found_files}")
        
        return len(found_files) > 0  # Success if we got at least one output file

# ============================================================================
# OUTPUT PRESERVATION AND PARSING
# ============================================================================

def preserve_outputs(row_dict, csv_filename, target_results_dir, individual_results_dir):
    """
    Preserve outputs for this run in a tree-specific directory.
    
    Args:
        row_dict: Dictionary with tree parameters (country, date, plot_id, tile_coords, tree_id)
        csv_filename: Name of input CSV file
        target_results_dir: Path to temporary results directory
        individual_results_dir: Path to save individual tree results
    """
    # Create main individual results directory if it doesn't exist
    individual_results_dir.mkdir(exist_ok=True)
    
    # Create tree-specific directory using country, date, plot_id, tile_coords and tree_id
    country = row_dict.get('country', 'unknown_country')
    date = row_dict.get('date', 'unknown_date')
    plot_id = row_dict.get('plot_id', 'unknown_plot')
    tile_coords = row_dict.get('tile_coords', 'unknown_tile')
    tree_id = row_dict.get('tree_id', 'unknown_tree')
    tree_specific_dir = individual_results_dir / f"{country}_{date}_{plot_id}_{tile_coords}_Tree_{tree_id}"
    tree_specific_dir.mkdir(exist_ok=True)
    
    # Copy all output files from results directory
    if target_results_dir.exists():
        for output_file in target_results_dir.glob("*"):
            if output_file.is_file():
                shutil.copy2(output_file, tree_specific_dir / output_file.name)

def parse_backscatter(target_results_dir):
    """
    Parse backscatter results from MIMICS output files.
    
    Extracts total and component backscatter values for all polarizations
    (VV, HH, VH, HV) and scattering mechanisms.
    
    Args:
        target_results_dir: Path to directory containing MIMICS output files
        
    Returns:
        DataFrame with backscatter results by angle and component
    """
    like_file = target_results_dir / "forest_sigma_like.out"
    cross_file = target_results_dir / "forest_sigma_cross.out"
    
    # Component column mappings
    components = {
        'total': (1, 2),
        'gnd_crown_gnd': (3, 4),
        'crown_ground': (5, 6),
        'ground_crown': (7, 8),
        'direct_crown': (9, 10),
        'trunk_ground': (11, 12),
        'ground_trunk': (13, 14),
        'direct_ground': (15, 16)
    }
    
    def parse_file(filename, is_cross=False):
        data = {}
        if not filename.exists():
            return data
            
        with open(filename, 'r') as f:
            lines = f.readlines()
        # Find where data starts (2 lines after header with 'Theta' and 'Total Backscatter')
        data_start = next((i + 2 for i, line in enumerate(lines) 
                          if 'Theta' in line and 'Total Backscatter' in line), -1)
        
        # Exit if no data section found
        if data_start == -1:
            return data
        
        # If data is found, process each data line
        for line in lines[data_start:]:
            parts = line.split() # line.split() splits the line into segments at each whitespace.
            try:
                if len(parts) >= 16:
                    theta = float(parts[0])
                    data[theta] = {}
                    # Extract values for each scattering component
                    for comp_name, (col1, col2) in components.items():
                        # Convert to None if value is missing data flag (-900)
                        val1 = float(parts[col1]) if float(parts[col1]) > -900 else None
                        val2 = float(parts[col2]) if float(parts[col2]) > -900 else None
                        # Label columns based on polarization type
                        if is_cross:
                            data[theta][f'{comp_name}_vh'] = val1
                            data[theta][f'{comp_name}_hv'] = val2
                        else:
                            data[theta][f'{comp_name}_vv'] = val1
                            data[theta][f'{comp_name}_hh'] = val2
            except (ValueError, IndexError):
                continue # Skip problem lines
        return data
    # Parse both output files
    like_data = parse_file(like_file, is_cross=False) # VV/HH polarizations
    cross_data = parse_file(cross_file, is_cross=True) # VH/HV polarizations
    
    # Combine results from both files
    results = []
    all_thetas = set(like_data.keys()) | set(cross_data.keys())

    # Create combined data for each angle
    for theta in sorted(all_thetas):
        row = {'theta': theta}
        if theta in like_data:
            row.update(like_data[theta])
        if theta in cross_data:
            row.update(cross_data[theta])
        results.append(row)
    
    return pd.DataFrame(results)

# ============================================================================
# PARALLEL PROCESSING WORKER FUNCTIONS
# ============================================================================

def run_single_model_with_retry_wrapper(args_with_retries):
    """
    Wrapper function to handle retry count in multiprocessing.
    
    Args:
        args_with_retries: Tuple of (work_args, max_retries)
        
    Returns:
        DataFrame with results or None if failed
    """
    *args, max_retries = args_with_retries
    return run_single_model_with_retry(args, max_retries)

def run_single_model_with_retry(args, max_retries=3):
    """
    Worker function to run model for one parameter set with retry logic.
    
    Args:
        args: tuple of (run_number, row_dict, csv_filename, preserve_flag, config_dir, src_dir, individual_results_dir)
    
    Returns:
        DataFrame with results for this run, or None if run failed
    """
    run_number, row_dict, csv_filename, preserve, config_dir, src_dir, individual_results_dir = args
    
    # Check for shutdown request
    if shutdown_requested:
        return None
    
    for attempt in range(max_retries):
        try:
            return run_single_model(args)
        except Exception as e:
            if attempt == max_retries - 1:
                # Final failure - log detailed error
                tree_id = row_dict.get('tree_id', 'unknown')
                plot_id = row_dict.get('plot_id', 'unknown')
                logging.error(f"Run {run_number} (Plot {plot_id}, Tree {tree_id}): Failed after {max_retries} attempts")
                logging.debug(f"  Error details: {e}")
                return None
            # Retry with exponential backoff
            logging.warning(f"Run {run_number}: Attempt {attempt + 1} failed, retrying... ({e})")
            time.sleep(2 ** attempt)
    
    return None

def run_single_model(args):
    """
    Worker function to run MIMICS model for one parameter set in isolation.
    
    This function:
    1. Creates isolated temporary directory for this worker
    2. Copies and configures MIMICS input files
    3. Executes MIMICS model
    4. Parses and returns results
    
    Args:
        args: Tuple of (run_number, row_dict, csv_filename, preserve_flag,
                       config_dir, src_dir, individual_results_dir)
    
    Returns:
        DataFrame with results for this run, or None if run failed
    """
    run_number, row_dict, csv_filename, preserve, config_dir, src_dir, individual_results_dir = args
    
    try:
        # Create temporary isolated directory for this worker
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Create isolated copies of model directories (optimized: only copy data, link code)
            temp_config_dir = temp_path / "data"
            temp_results_dir = temp_path / "results"
            
            # Copy only configuration directory (data gets modified)
            shutil.copytree(config_dir, temp_config_dir)
            temp_results_dir.mkdir()
            
            # Configure MIMICS branches based on tree parameters
            config_file = temp_config_dir / "configuration.input"
            configure_mimics_branches(config_file, row_dict)
            
            # Set parameters in isolated directory
            set_parameters(row_dict, temp_config_dir)
            
            # Run model in isolated directory using original source directory
            if run_model(src_dir, temp_config_dir, temp_results_dir):
                # Debug: Check what files were actually created
                if temp_results_dir.exists():
                    result_files = list(temp_results_dir.glob("*"))
                    logging.debug(f"Run {run_number}: Found {len(result_files)} files in results dir: {[f.name for f in result_files]}")
                else:
                    logging.debug(f"Run {run_number}: Results directory doesn't exist: {temp_results_dir}")
                
                # Parse results from isolated directory
                result = parse_backscatter(temp_results_dir)
                
                if result.empty:
                    plot_id = row_dict.get('plot_id', 'unknown')
                    tree_id = row_dict.get('tree_id', 'unknown')
                    logging.warning(f"Run {run_number} (Plot {plot_id}, Tree {tree_id}): No backscatter data parsed from output files")
                    
                    # Additional debugging - check if files exist but parsing failed
                    like_file = temp_results_dir / "forest_sigma_like.out"
                    cross_file = temp_results_dir / "forest_sigma_cross.out"
                    logging.debug(f"  like_file exists: {like_file.exists()}, cross_file exists: {cross_file.exists()}")
                    
                    if like_file.exists():
                        with open(like_file, 'r') as f:
                            lines = f.readlines()
                        logging.debug(f"  like_file has {len(lines)} lines")
                        if len(lines) < 10:
                            logging.debug(f"  → Output file may be incomplete or corrupted")
                    else:
                        logging.debug(f"  → MIMICS did not generate expected output files")
                    return None
                
                # Add input parameters to output with priority columns first
                priority_cols = ['country', 'date', 'plot_id', 'tile_coords', 'tree_id']
                for col in priority_cols:
                    if col in row_dict:
                        result[col] = row_dict[col]
                
                # Add remaining parameters
                for param, value in row_dict.items():
                    if param not in priority_cols:
                        result[param] = value
                result['run'] = run_number - 1  # 0-indexed
                
                # Reorder columns to put priority columns first
                existing_cols = [col for col in priority_cols if col in result.columns]
                other_cols = [col for col in result.columns if col not in priority_cols]
                result = result[existing_cols + other_cols]
                
                # Preserve outputs if requested
                if preserve:
                    preserve_outputs(row_dict, csv_filename, temp_results_dir, individual_results_dir)
                
                return result
            else:
                plot_id = row_dict.get('plot_id', 'unknown')
                tree_id = row_dict.get('tree_id', 'unknown')
                logging.error(f"Run {run_number} (Plot {plot_id}, Tree {tree_id}): MIMICS model execution failed")
                logging.debug(f"  → Check MIMICS configuration and input parameters")
                return None
                
    except Exception as e:
        plot_id = row_dict.get('plot_id', 'unknown')
        tree_id = row_dict.get('tree_id', 'unknown')
        logging.error(f"Run {run_number} (Plot {plot_id}, Tree {tree_id}): Unexpected error - {str(e)}")
        logging.debug(f"  Error type: {type(e).__name__}")
        return None

# ============================================================================
# COMMAND LINE INTERFACE AND MAIN WORKFLOW
# ============================================================================

def main():
    """
    Main entry point for MIMICS model execution.
    
    Workflow:
    1. Parse command line arguments
    2. Validate paths and configuration
    3. Load input CSV with parameter sets
    4. Process parameter sets in parallel
    5. Combine and save results to output CSV
    """
    
    # ========================================================================
    # STEP 1: Set up signal handlers and parse arguments
    # ========================================================================
    # Set up signal handlers for graceful shutdown
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    parser = argparse.ArgumentParser(
        description='Run MIMICS model with parameter sets from the input file',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
        epilog="""
Examples:
  %(prog)s -i model_input.csv -c ./model/data -s ./model/code
        """
    )
    
    # Required arguments
    parser.add_argument(
        '-i', '--input',
        type=str,
        required=True,
        help='Path to CSV file containing MIMICS model input parameters (required)'
    )
    
    parser.add_argument(
        '-o', '--output',
        type=str,
        default=None,
        help='Path to output CSV file for combined results of MIMICS output (optional). If not provided, will use same directory as input file with "mimics_inputs_" replaced by "mimics_outputs_" in filename.'
    )
    
    # Path arguments with defaults
    parser.add_argument(
        '-c', '--mimics-config',
        type=str,
        default="./model/data",
        help=f'Path to MIMICS model configuration directory'
    )
    
    parser.add_argument(
        '-s', '--mimics-src',
        type=str,
        default="./model/code",
        help=f'Path to MIMICS model source directory containing mimics1.5 executable'
    )
    
    # Processing options
    parser.add_argument(
        '--n_cores',
        type=int,
        default=max(1, cpu_count() - 2),
        help='Number of parallel processes to use'
    )
    
    parser.add_argument(
        '--preserve-individual-results',
        action='store_true',
        help='Preserve individual run outputs in separate directories for each tree'
    )
    
    parser.add_argument(
        '--retries',
        type=int,
        default=3,
        help='Number of retry attempts for failed runs'
    )
    
    parser.add_argument(
        '--debug',
        action='store_true',
        help='Enable DEBUG level logging in log file (default: only INFO, WARNING, ERROR)'
    )
    
    args = parser.parse_args()
    
    # ========================================================================
    # STEP 2: Resolve paths and set up logging
    # ========================================================================
    # Convert input path to absolute Path object first
    input_file = Path(args.input).resolve()
    
    # Auto-generate output path if not provided
    if args.output is None:
        # Get the input filename and replace 'mimics_inputs_' with 'mimics_outputs_'
        input_filename = input_file.name
        if 'mimics_inputs_' in input_filename:
            output_filename = input_filename.replace('mimics_inputs_', 'mimics_outputs_')
        else:
            # If pattern not found, just prepend 'output_' to the original filename
            output_filename = f'output_{input_filename}'
        
        # Use same directory as input file
        output_path = input_file.parent / output_filename
        logging_note = " (auto-generated from input filename)"
    else:
        output_path = Path(args.output)
        logging_note = ""
    
    # Create output directory if it doesn't exist (needed for log file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # Set up logging with two handlers:
    # 1. File handler (INFO/DEBUG level based on --debug flag) - captures important/all information
    # 2. Console handler (INFO level) - shows only key information
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_path = output_path.parent / f"{output_path.stem}_{timestamp}.log"
    
    # Create logger
    logger = logging.getLogger()
    logger.setLevel(logging.DEBUG)  # Capture all levels at root
    
    # File handler - logs based on debug flag
    file_handler = logging.FileHandler(log_file_path)
    if args.debug:
        # Debug mode: log everything (DEBUG and above)
        file_handler.setLevel(logging.DEBUG)
    else:
        # Normal mode: log INFO and above (skips DEBUG messages)
        file_handler.setLevel(logging.INFO)
    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(file_formatter)
    
    # Console handler - logs only important info (INFO and above)
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = logging.Formatter('%(message)s')  # Simpler format for console
    console_handler.setFormatter(console_formatter)
    
    # Add handlers to logger
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    # ========================================================================
    # STEP 3: Validate paths and configuration
    # ========================================================================
    # Convert paths to Path objects
    config_dir = Path(args.mimics_config)
    src_dir = Path(args.mimics_src)
    output_file = output_path  # Already resolved above
    
    # Create individual results directory
    individual_results_dir = output_file.parent / "individual_tree_results"
    
    # Validate paths
    if not config_dir.exists():
        logging.error(f"✗ MIMICS configuration directory not found: {config_dir}")
        logging.error(f"  → Please check that the path exists and is accessible")
        return 1
    
    if not src_dir.exists():
        logging.error(f"✗ MIMICS source directory not found: {src_dir}")
        logging.error(f"  → Please check that the path exists and is accessible")
        return 1
    
    if not input_file.exists():
        logging.error(f"✗ Input CSV file not found: {input_file}")
        logging.error(f"  → Please verify the file path is correct")
        return 1
    
    # Check for mimics executable
    mimics_exe = src_dir / "mimics1.5"
    if not mimics_exe.exists():
        logging.error(f"✗ MIMICS executable not found: {mimics_exe}")
        logging.error(f"  → Expected 'mimics1.5' in source directory")
        logging.error(f"  → Please ensure MIMICS is properly installed")
        return 1
    
    print("\n" + "=" * 80)
    print("MIMICS MODEL EXECUTION")
    print("=" * 80)
    logging.info("\n[1/7] Configuration")
    logging.info(f"  MIMICS config: {config_dir}")
    logging.info(f"  MIMICS source: {src_dir}")
    logging.info(f"  Input file: {input_file}")
    logging.info(f"  Output file: {output_file}{logging_note}")
    logging.info(f"  Log file: {log_file_path.name}")
    logging.info(f"  Log level: {'DEBUG (detailed)' if args.debug else 'INFO (normal)'}")
    logging.info(f"  Parallel cores: {args.n_cores}")
    logging.info(f"  Preserve individual results: {args.preserve_individual_results}")
    logging.info(f"  Max retries: {args.retries}")
    if args.preserve_individual_results:
        logging.info(f"  Individual results directory: {individual_results_dir}")
    
    # ========================================================================
    # STEP 4: Load and validate CSV data
    # ========================================================================
    logging.info(f"\n[2/7] Loading input CSV file...")
    try:
        df = pd.read_csv(input_file)
        logging.debug(f"CSV raw shape: {df.shape}")
        
        # Remove completely empty rows
        df = df.dropna(how='all')
        logging.info(f"  Loaded: {df.shape[0]} parameter sets × {df.shape[1]} columns")
        
        if df.empty:
            logging.error(f"✗ No valid data rows found in CSV file")
            logging.error(f"  → File appears to be empty or contains only headers")
            return 1
        
        # Validate required columns
        required_cols = ['plot_id', 'tile_coords', 'tree_id']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            logging.error(f"✗ Missing required columns: {missing_cols}")
            logging.error(f"  → CSV must contain: {required_cols}")
            logging.error(f"  → Found columns: {list(df.columns)}")
            return 1
        
        # Check for parameter columns
        param_cols = [col for col in df.columns if col in PARAMETERS]
        if not param_cols:
            logging.warning(f"⚠ No recognized MIMICS parameters found in CSV")
            logging.warning(f"  → Expected parameters like: frequency, soil_moisture, branch_1_density, etc.")
        else:
            logging.info(f"  ✓ Found {len(param_cols)} MIMICS parameter columns")
            logging.debug(f"  Parameter columns: {param_cols}")
        
    except Exception as e:
        logging.error(f"✗ Error loading CSV file: {e}")
        logging.error(f"  → Please verify the file format is valid CSV")
        return 1
    
    # ========================================================================
    # STEP 5: Prepare work items for parallel processing
    # ========================================================================
    logging.info(f"\n[3/7] Preparing parallel processing...")
    work_items = []
    for i, (_, row) in enumerate(df.iterrows()):
        row_dict = row.to_dict()
        work_items.append((i + 1, row_dict, input_file, args.preserve_individual_results, 
                          config_dir, src_dir, individual_results_dir))
    
    logging.info(f"  Total parameter sets: {len(work_items)}")
    logging.info(f"  Worker processes: {args.n_cores}")
    logging.info(f"  Expected outputs: {len(work_items)} × backscatter angles")

    # Add retry count to work items
    work_items_with_retries = [(item + (args.retries,)) for item in work_items]
    
    # ========================================================================
    # STEP 6: Run MIMICS model in parallel with progress tracking
    # ========================================================================
    logging.info(f"\n[4/7] Running MIMICS model...")
    start_time = time.time()
    
    # Run in parallel using multiprocessing with progress bar
    try:
        with Pool(processes=args.n_cores) as pool:
            # Use tqdm for progress tracking
            with tqdm(total=len(work_items_with_retries), desc="Processing MIMICS runs", unit="run") as pbar:
                results = []
                for result in pool.imap(run_single_model_with_retry_wrapper, work_items_with_retries):
                    if shutdown_requested:
                        logging.warning("\n⚠ Shutdown requested by user")
                        logging.info("  → Terminating remaining jobs and cleaning up...")
                        pool.terminate()
                        break
                    results.append(result)
                    pbar.update(1)
                    
                    # Update progress description with success count
                    successful = len([r for r in results if r is not None])
                    pbar.set_description(f"Processing MIMICS runs (successful: {successful})")
    
    except KeyboardInterrupt:
        logging.warning("\n⚠ Processing interrupted by user (Ctrl+C)")
        logging.info("  → Partial results may be available")
        return 1
    except Exception as e:
        logging.error(f"\n✗ Error during parallel processing: {e}")
        logging.error(f"  → This may indicate a system resource issue or MIMICS crash")
        return 1
    
    elapsed_time = time.time() - start_time
    logging.info(f"\n[5/7] Processing completed in {elapsed_time:.1f} seconds")
    
    # ========================================================================
    # STEP 7: Combine and save results
    # ========================================================================
    logging.info(f"\n[6/7] Analyzing results...")
    
    # Filter out None results (failed runs) and collect successful ones
    all_results = [r for r in results if r is not None]
    successful_count = len(all_results)
    failed_count = len(work_items) - successful_count
    success_rate = (successful_count / len(work_items) * 100) if len(work_items) > 0 else 0
    
    logging.info(f"  Successful runs: {successful_count}/{len(work_items)} ({success_rate:.1f}%)")
    if failed_count > 0:
        logging.warning(f"  ⚠ Failed runs: {failed_count}")
        logging.warning(f"    → Check log file for details: {log_file_path.name}")
    
    # Save combined results
    if all_results:
        logging.info(f"\n[7/7] Saving results...")
        try:
            combined = pd.concat(all_results, ignore_index=True)
            
            # Get summary statistics
            unique_trees = combined[['plot_id', 'tile_coords', 'tree_id']].drop_duplicates()
            num_unique_trees = len(unique_trees)
            num_angles = combined['theta'].nunique() if 'theta' in combined.columns else 0
            total_rows = len(combined)
            
            combined.to_csv(output_file, index=False)
            file_size_mb = output_file.stat().st_size / (1024 * 1024)
            
            logging.info(f"  ✓ Output saved: {output_file.name}")
            logging.info(f"  File size: {file_size_mb:.2f} MB")
            logging.info(f"  Total rows: {total_rows:,}")
            logging.info(f"  Unique trees: {num_unique_trees}")
            logging.info(f"  Angles per run: {num_angles}")
            
            if args.preserve_individual_results:
                num_tree_dirs = len(list(individual_results_dir.glob("*"))) if individual_results_dir.exists() else 0
                logging.info(f"  ✓ Individual results: {num_tree_dirs} tree directories")
                logging.info(f"  Location: {individual_results_dir}")
            
            # Final summary
            print("\n" + "=" * 80)
            print("EXECUTION SUMMARY")
            print("=" * 80)
            print(f"Status: {'✓ SUCCESS' if failed_count == 0 else '⚠ COMPLETED WITH WARNINGS'}")
            print(f"Successful runs: {successful_count}/{len(work_items)} ({success_rate:.1f}%)")
            if failed_count > 0:
                print(f"Failed runs: {failed_count} (see log for details)")
            print(f"Processing time: {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)")
            print(f"Average time per run: {elapsed_time/len(work_items):.2f} seconds")
            print(f"\nOutput:")
            print(f"  Main results: {output_file}")
            print(f"  Log file: {log_file_path}")
            if args.preserve_individual_results:
                print(f"  Individual results: {individual_results_dir}")
            print("=" * 80 + "\n")
            
            logging.info("\n" + "=" * 80)
            logging.info("EXECUTION COMPLETED SUCCESSFULLY" if failed_count == 0 else "EXECUTION COMPLETED WITH WARNINGS")
            logging.info("=" * 80)
            
            return 0
            
        except Exception as e:
            logging.error(f"\n✗ Error saving results: {e}")
            logging.error(f"  → This may indicate a file permission or disk space issue")
            return 1
    else:
        logging.error(f"\n✗ No successful runs to save")
        logging.error(f"  → All {len(work_items)} parameter sets failed")
        logging.error(f"  → Common causes:")
        logging.error(f"    1. MIMICS executable not working properly")
        logging.error(f"    2. Invalid parameter values in input CSV")
        logging.error(f"    3. Missing required input files in config directory")
        logging.error(f"  → Check detailed error messages in log file: {log_file_path}")
        return 1

if __name__ == "__main__":
    main()
